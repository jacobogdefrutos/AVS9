{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_SAM = sam_model_registry[\"vit_h\"](checkpoint=\"/home/jacobo15defrutos/AVS9/sam_vit_h_4b8939.pth\")\n",
    "model_SAM.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_number_in_string(input_string):\n",
    "    # Regular expression to find a number in a string\n",
    "    pattern = r'\\d+'\n",
    "\n",
    "    # Search for the pattern in the input string\n",
    "    match = re.search(pattern, input_string)\n",
    "\n",
    "    # Check if a match is found\n",
    "    if match:\n",
    "        # Extract and return the matched number\n",
    "        return int(match.group())\n",
    "\n",
    "    # Return None if no number is found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing:  Imag\n",
      "Captura_de_pantalla_(35).png\n",
      "/home/jacobo15defrutos/AVS9/Data/Data_seg_SAM/train/Imag/Captura_de_pantalla_(35).png\n",
      "Captura_de_pantalla_(54).png\n",
      "/home/jacobo15defrutos/AVS9/Data/Data_seg_SAM/train/Imag/Captura_de_pantalla_(54).png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import os\n",
    "\n",
    "data_folder_train= '/home/jacobo15defrutos/AVS9/Data/Data_seg_SAM/train'\n",
    "data_folder_val= '/home/jacobo15defrutos/AVS9/Data/Data_seg_SAM/val'\n",
    "transformed__train_data_imgs= defaultdict(dict)\n",
    "transformed__train_data_masks= defaultdict(dict)\n",
    "list_numbers=[]\n",
    "for folder in os.listdir(data_folder_train):\n",
    "    print(\"-\"*20)\n",
    "    print(\"Preprocessing: \",folder)\n",
    "    for file in os.listdir(f'{data_folder_train}/{folder}'):\n",
    "      print(file)\n",
    "      if 'png'  in file:\n",
    "        path = f'{data_folder_train}/{folder}/{file}'\n",
    "        print(path)\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (800, 800))\n",
    "        if 'Imag'  in folder:\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "          #print(image.shape)\n",
    "          transform = ResizeLongestSide(model_SAM.image_encoder.img_size)\n",
    "          #print(\"mierda: \", model_SAM.model.image_encoder.img_size)\n",
    "          input_image = transform.apply_image(image)\n",
    "          input_image_torch = torch.as_tensor(input_image, device=device,dtype=torch.float32)\n",
    "          #print(input_image_torch.shape)\n",
    "          #expected_size = (1, 3, model_SAM.model.image_encoder.img_size, model_SAM.model.image_encoder.img_size)\n",
    "          #transformed_image = input_image_torch.view(expected_size)\n",
    "          transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "          #print(transformed_image.shape)\n",
    "          input_image = model_SAM.image_encoder(transformed_image)#[1,256,64,64]\n",
    "          original_image_size = image.shape[:2]\n",
    "          #print(\"Original: \", original_image_size)\n",
    "          input_size = tuple(transformed_image.shape[-2:])\n",
    "          number= find_number_in_string(file)\n",
    "          #print(\"nUmber: \",number)\n",
    "          list_numbers.append(number)\n",
    "          transformed__train_data_imgs[number]['image'] = input_image\n",
    "          transformed__train_data_imgs[number]['input_size'] = input_size\n",
    "          transformed__train_data_imgs[number]['original_image_size'] = original_image_size\n",
    "        elif 'Labels'in folder:\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "          image= image[:,:,1]\n",
    "          #print(image.shape)\n",
    "          transform = ResizeLongestSide(model_SAM.image_encoder.img_size)\n",
    "          #print(\"mierda: \", model_SAM.model.image_encoder.img_size)\n",
    "          input_image = transform.apply_image(image)\n",
    "          input_image_torch = torch.as_tensor(input_image, device=device,dtype=torch.uint8)\n",
    "          input_image_torch= input_image_torch.unsqueeze(2)\n",
    "          #print(input_image_torch.shape)\n",
    "          #expected_size = (1, 3, model_SAM.model.image_encoder.img_size, model_SAM.model.image_encoder.img_size)\n",
    "          #transformed_image = input_image_torch.view(expected_size)\n",
    "          transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "          #print(transformed_image.shape)\n",
    "          #input_image = model_SAM.model.image_encoder(transformed_image)\n",
    "          original_image_size = image.shape[:2]\n",
    "          #print(\"Original: \", original_image_size)\n",
    "          input_size = tuple(transformed_image.shape[-2:])\n",
    "          number= find_number_in_string(file)\n",
    "          #print(\"nUmber: \",number)\n",
    "          transformed__train_data_masks[number]['image'] = transformed_image\n",
    "          #transformed__train_data_masks[number]['input_size'] = input_size\n",
    "          #transformed__train_data_masks[number]['original_image_size'] = original_image_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
